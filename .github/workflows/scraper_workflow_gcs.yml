name: TrovaCasa Scraper with GCS

on:
  # Esecuzione automatica ogni 2 giorni alle 19:30 UTC
  schedule:
    - cron: '30 19 */2 * *'
  
  # Permetti esecuzione manuale
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Numero massimo di pagine da scaricare (0 = tutte, consigliato max 10 per test)'
        required: false
        default: '3'
        type: string

jobs:
  scrape-to-gcs:
    runs-on: ubuntu-latest
    timeout-minutes: 180 # Tempo di timeout di 3 ore
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-gcs-${{ hashFiles('**/requirements_gcs.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-gcs-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements_gcs.txt
    
    - name: Setup Google Cloud credentials
      env:
        GCP_SA_KEY: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}
      run: |
        echo "üîß Configurazione credenziali Google Cloud Storage"
        # Crea il file delle credenziali in modo sicuro
        echo "$GCP_SA_KEY" | base64 -d > /tmp/gcs_key.json || echo "$GCP_SA_KEY" > /tmp/gcs_key.json
        # Verifica che il file JSON sia valido
        if python3 -m json.tool /tmp/gcs_key.json > /dev/null 2>&1; then
          echo "‚úÖ Credenziali JSON valide"
        else
          echo "‚ùå Errore: credenziali JSON non valide"
          exit 1
        fi
        # Imposta le variabili d'ambiente
        echo "GOOGLE_APPLICATION_CREDENTIALS=/tmp/gcs_key.json" >> $GITHUB_ENV
        echo "GCP_BUCKET_NAME=${{ secrets.GCP_BUCKET_NAME }}" >> $GITHUB_ENV
    
    - name: Run scraper with GCS upload
      env:
        MAX_PAGES: ${{ github.event.inputs.max_pages || '0' }} # Se non viene specificato un numero di pagine massimo esegue scraping su tutte le pagine (valore 0)
      run: |
        echo "üöÄ Avvio scraping con upload su GCS"
        echo "üìä MAX_PAGES: $MAX_PAGES"
        echo "ü™£ GCS Bucket: $GCP_BUCKET_NAME"
        python scraper_completo_gcs.py
    
    - name: Read GCS upload info
      id: gcs_info
      run: |
        if [ -f gcs_info.txt ]; then
          echo "üìä Lettura informazioni upload GCS"
          
          # Leggi le variabili dal file
          while IFS='=' read -r key value; do
            case $key in
              GCS_URL)
                echo "gcs_url=$value" >> $GITHUB_OUTPUT
                ;;
              FILENAME)
                echo "filename=$value" >> $GITHUB_OUTPUT
                ;;
              ANNUNCI_COUNT)
                echo "annunci_count=$value" >> $GITHUB_OUTPUT
                ;;
              TIMESTAMP)
                echo "timestamp=$value" >> $GITHUB_OUTPUT
                ;;
            esac
          done < gcs_info.txt
          
          echo "‚úÖ Informazioni GCS caricate correttamente"
        else
          echo "‚ùå File gcs_info.txt non trovato"
          echo "gcs_url=ERROR" >> $GITHUB_OUTPUT
          echo "filename=ERROR" >> $GITHUB_OUTPUT
          echo "annunci_count=0" >> $GITHUB_OUTPUT
          echo "timestamp=ERROR" >> $GITHUB_OUTPUT
        fi
    
    - name: Create summary report
      run: |
        echo "# üìä Report Scraping TrovaCasa Milano" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**üìÖ Timestamp**: ${{ steps.gcs_info.outputs.timestamp }}" >> $GITHUB_STEP_SUMMARY
        echo "**üè† Annunci processati**: ${{ steps.gcs_info.outputs.annunci_count }}" >> $GITHUB_STEP_SUMMARY
        echo "**üìÅ File creato**: ${{ steps.gcs_info.outputs.filename }}" >> $GITHUB_STEP_SUMMARY
        echo "**‚òÅÔ∏è URL GCS**: ${{ steps.gcs_info.outputs.gcs_url }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üîó Accesso ai dati" >> $GITHUB_STEP_SUMMARY
        echo "I dati sono stati caricati privatamente su Google Cloud Storage." >> $GITHUB_STEP_SUMMARY
        echo "Per accedere ai dati, utilizzare la Google Cloud Console o il CLI." >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üìã Struttura dati" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
        echo "_id;url;prezzo;titolo;indirizzo;superficie_m2;num_locali;num_bagni;classe_ener;tags;attivo;data_comparsa;data_aggiornamento;data_scomparsa" >> $GITHUB_STEP_SUMMARY
        echo '```' >> $GITHUB_STEP_SUMMARY
    
    - name: Cleanup temporary files
      run: |
        rm -f /tmp/gcs_key.json
        rm -f gcs_info.txt
    
    - name: Log completion
      run: |
        echo "‚úÖ Scraping completato con successo!"
        echo "üè† Annunci processati: ${{ steps.gcs_info.outputs.annunci_count }}"
        echo "‚òÅÔ∏è Caricato su GCS: ${{ steps.gcs_info.outputs.gcs_url }}"
